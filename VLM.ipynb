{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6KRec3rcfPKHoHyXf2WzI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daviduche03/Applied-Deep-Learning/blob/main/VLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo\n",
        "import torchinfo\n"
      ],
      "metadata": {
        "id": "re8Y3B4YxxLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNSharh8zpZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33804723-3301-4b36-fd59-655eff65cffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Patch Output Shape: torch.Size([1, 196, 768])\n",
            "MLP Output Shape: torch.Size([1, 196, 512])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class ImagePatch(torch.nn.Module):\n",
        "    def __init__(self, patch_size):\n",
        "        super(ImagePatch, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Unfold the image into patches of size (batch, channels, num_patches_x, num_patches_y, patch_size, patch_size)\n",
        "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
        "\n",
        "        # Permute to bring patches to the second dimension and flatten each patch\n",
        "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous()  # (batch, num_patches_x, num_patches_y, channels, patch_size, patch_size)\n",
        "\n",
        "        # Reshape to (batch, num_patches, channels * patch_size * patch_size)\n",
        "        x = x.view(x.size(0), -1, self.patch_size * self.patch_size * x.size(3))  # (batch, num_patches, flattened_patch_size)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example usage for ImagePatch\n",
        "image_patch = ImagePatch(16)\n",
        "input_image = torch.randn(1, 3, 224, 224)  # 1 image, 3 channels, 224x224\n",
        "output = image_patch(input_image)\n",
        "print(\"Image Patch Output Shape:\", output.shape)  # Expected output shape: (1, 196, 768)\n",
        "\n",
        "\n",
        "class VIT(torch.nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, nheads=8, num_layers=6, num_class=10):\n",
        "        super(VIT, self).__init__()\n",
        "        self.linear_proj = torch.nn.Linear(in_dim, out_dim)\n",
        "        self.pos_encoding = torch.nn.Parameter(torch.randn(1, 196, out_dim))\n",
        "        encoder_layer = torch.nn.TransformerEncoderLayer(d_model=out_dim, nhead=nheads)\n",
        "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.classifier = torch.nn.Linear(out_dim, num_class)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear_proj(x) + self.pos_encoding\n",
        "        x = self.transformer_encoder(x)\n",
        "        # class_token = x[:, 0, :]\n",
        "        # x = self.classifier(class_token)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example usage for VIT\n",
        "mlp = VIT(in_dim=768, out_dim=512)  # in_dim now matches the flattened patch size\n",
        "mlp_output = mlp(output)\n",
        "print(\"MLP Output Shape:\", mlp_output.shape)  # Expected output shape: (1, num_class)\n",
        "\n",
        "class customDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, data, labels, transform=None):\n",
        "    self.data = data\n",
        "    self.labels = labels\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img, label =  self.data[idx], self.labels[idx]\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "    return img, label\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class ImagePatch(torch.nn.Module):\n",
        "    def __init__(self, patch_size):\n",
        "        super(ImagePatch, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Unfold the image into patches\n",
        "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
        "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
        "        x = x.view(x.size(0), -1, self.patch_size * self.patch_size * x.size(3))\n",
        "        return x\n",
        "\n",
        "class VIT(torch.nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, nheads=8, num_layers=6):\n",
        "        super(VIT, self).__init__()\n",
        "        self.linear_proj = torch.nn.Linear(in_dim, out_dim)\n",
        "        self.pos_encoding = torch.nn.Parameter(torch.randn(1, 196, out_dim))\n",
        "        encoder_layer = torch.nn.TransformerEncoderLayer(d_model=out_dim, nhead=nheads)\n",
        "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear_proj(x) + self.pos_encoding\n",
        "        x = self.transformer_encoder(x)\n",
        "        return x\n",
        "\n",
        "class MiniTransformer(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, nheads, num_layers):\n",
        "        super(MiniTransformer, self).__init__()\n",
        "        self.token_embedding = torch.nn.Embedding(vocab_size, embed_dim, padding_idx=0)  # Added padding_idx\n",
        "        self.pos_encoding = torch.nn.Parameter(torch.randn(1, 512, embed_dim))\n",
        "        encoder_layer = torch.nn.TransformerEncoderLayer(d_model=embed_dim, nhead=nheads)\n",
        "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, text_tokens):\n",
        "        # Ensure input tokens are within vocab range\n",
        "        text_tokens = torch.clamp(text_tokens, min=0, max=self.token_embedding.num_embeddings - 1)\n",
        "        pos_encoding_slice = self.pos_encoding[:, :text_tokens.size(1), :]\n",
        "        x = self.token_embedding(text_tokens) + pos_encoding_slice\n",
        "        x = self.transformer_encoder(x)\n",
        "        return x\n",
        "\n",
        "class MultiModalTransformer(torch.nn.Module):\n",
        "    def __init__(self, embed_dim, nheads, num_layers):\n",
        "        super(MultiModalTransformer, self).__init__()\n",
        "        encoder = torch.nn.TransformerEncoderLayer(d_model=embed_dim, nhead=nheads)\n",
        "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, image_features, text_features):\n",
        "        # Add sequence dimension if missing\n",
        "        if len(image_features.shape) == 2:\n",
        "            image_features = image_features.unsqueeze(0)\n",
        "        if len(text_features.shape) == 2:\n",
        "            text_features = text_features.unsqueeze(0)\n",
        "\n",
        "        print(f\"Image2 Features Shape: {image_features.shape}\")\n",
        "        print(f\"Text2 Features Shape: {text_features.shape}\")\n",
        "\n",
        "        x = torch.cat((image_features, text_features), dim=1)\n",
        "        print(f\"MultiModalTransformer Input Shape: {x.shape}\")\n",
        "        x = self.transformer_encoder(x)\n",
        "        return x\n",
        "\n",
        "class NextTokenPredictor(torch.nn.Module):\n",
        "    def __init__(self, embed_dim, vocab_size):\n",
        "        super(NextTokenPredictor, self).__init__()\n",
        "        self.linear = torch.nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "class VisionLanguageModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, image_embed_dim, text_embed_dim, nheads, num_layers):\n",
        "        super(VisionLanguageModel, self).__init__()\n",
        "        self.image_patch = ImagePatch(16)\n",
        "        self.image_encoder = VIT(in_dim=image_embed_dim, out_dim=text_embed_dim)  # Match text embedding dim\n",
        "        self.text_encoder = MiniTransformer(vocab_size, text_embed_dim, nheads, num_layers)\n",
        "        self.fusion_transformer = MultiModalTransformer(text_embed_dim, nheads, num_layers)\n",
        "        self.next_token_predictor = NextTokenPredictor(text_embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, image, text_tokens):\n",
        "        image_patches = self.image_patch(image)\n",
        "        image_features = self.image_encoder(image_patches)\n",
        "        text_features = self.text_encoder(text_tokens)\n",
        "        print(f\"Image Features Shape: {image_features.shape}\")\n",
        "        print(f\"Text Features Shape: {text_features.shape}\")\n",
        "        fusion_features = self.fusion_transformer(image_features, text_features)\n",
        "        print(f\"Fusion Features Shape: {fusion_features.shape}\")\n",
        "        next_token_logits = self.next_token_predictor(fusion_features)\n",
        "        return next_token_logits\n",
        "\n",
        "# Example usage\n",
        "def test_model():\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", token=\"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\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    vocab_size = tokenizer.vocab_size\n",
        "\n",
        "    # Create model\n",
        "    model = VisionLanguageModel(\n",
        "        vocab_size=vocab_size,\n",
        "        image_embed_dim=768,\n",
        "        text_embed_dim=512,\n",
        "        nheads=8,\n",
        "        num_layers=6\n",
        "    )\n",
        "\n",
        "\n",
        "    # Create dummy inputs\n",
        "    image = torch.randn(1, 3, 224, 224)\n",
        "    text = \"This is an example text.\"\n",
        "    text_tokens = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "    print(torchinfo.summary(model=model, input_data=(image, text_tokens)))\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(image, text_tokens)\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    return output\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t41c5dCBnOxl",
        "outputId": "8fa2da19-16c0-483a-9bb0-20d9a9d46ca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Features Shape: torch.Size([1, 196, 512])\n",
            "Text Features Shape: torch.Size([1, 512, 512])\n",
            "Image2 Features Shape: torch.Size([1, 196, 512])\n",
            "Text2 Features Shape: torch.Size([1, 512, 512])\n",
            "MultiModalTransformer Input Shape: torch.Size([1, 708, 512])\n",
            "Fusion Features Shape: torch.Size([1, 708, 512])\n",
            "=========================================================================================================\n",
            "Layer (type:depth-idx)                                  Output Shape              Param #\n",
            "=========================================================================================================\n",
            "VisionLanguageModel                                     [1, 708, 32000]           --\n",
            "├─ImagePatch: 1-1                                       [1, 196, 768]             --\n",
            "├─VIT: 1-2                                              [1, 196, 512]             100,352\n",
            "│    └─Linear: 2-1                                      [1, 196, 512]             393,728\n",
            "│    └─TransformerEncoder: 2-2                          [1, 196, 512]             --\n",
            "│    │    └─ModuleList: 3-1                             --                        18,914,304\n",
            "├─MiniTransformer: 1-3                                  [1, 512, 512]             262,144\n",
            "│    └─Embedding: 2-3                                   [1, 512, 512]             16,384,000\n",
            "│    └─TransformerEncoder: 2-4                          [1, 512, 512]             --\n",
            "│    │    └─ModuleList: 3-2                             --                        18,914,304\n",
            "├─MultiModalTransformer: 1-4                            [1, 708, 512]             --\n",
            "│    └─TransformerEncoder: 2-5                          [1, 708, 512]             --\n",
            "│    │    └─ModuleList: 3-3                             --                        18,914,304\n",
            "├─NextTokenPredictor: 1-5                               [1, 708, 32000]           --\n",
            "│    └─Linear: 2-6                                      [1, 708, 32000]           16,416,000\n",
            "=========================================================================================================\n",
            "Total params: 90,299,136\n",
            "Trainable params: 90,299,136\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 71.03\n",
            "=========================================================================================================\n",
            "Input size (MB): 0.61\n",
            "Forward/backward pass size (MB): 427.75\n",
            "Params size (MB): 284.10\n",
            "Estimated Total Size (MB): 712.45\n",
            "=========================================================================================================\n",
            "Image Features Shape: torch.Size([1, 196, 512])\n",
            "Text Features Shape: torch.Size([1, 512, 512])\n",
            "Image2 Features Shape: torch.Size([1, 196, 512])\n",
            "Text2 Features Shape: torch.Size([1, 512, 512])\n",
            "MultiModalTransformer Input Shape: torch.Size([1, 708, 512])\n",
            "Fusion Features Shape: torch.Size([1, 708, 512])\n",
            "Output shape: torch.Size([1, 708, 32000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import AutoTokenizer\n",
        "import json\n",
        "import os\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "class VisionLanguageDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 data_dir: str,\n",
        "                 annotations_file: str,\n",
        "                 tokenizer,\n",
        "                 max_length: int = 512,\n",
        "                 transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir (str): Directory with all the images\n",
        "            annotations_file (str): JSON file with format:\n",
        "                [\n",
        "                    {\n",
        "                        \"image_file\": \"image1.jpg\",\n",
        "                        \"caption\": \"a photo of a dog running\",\n",
        "                        \"next_tokens\": \"in the park\"\n",
        "                    },\n",
        "                    ...\n",
        "                ]\n",
        "            tokenizer: HuggingFace tokenizer\n",
        "            max_length (int): Maximum length of text sequence\n",
        "            transform: Optional transform to be applied on images\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.annotations = json.load(open(annotations_file))\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.transform = transform or transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                              std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      try:\n",
        "          # Load annotation\n",
        "          ann = self.annotations[idx]\n",
        "\n",
        "          # Load and transform image\n",
        "          image_path = os.path.join(self.data_dir, ann['image_file'])\n",
        "          image = Image.open(image_path).convert('RGB')\n",
        "          image = self.transform(image)\n",
        "\n",
        "          # Tokenize input caption\n",
        "          input_tokens = self.tokenizer(\n",
        "              ann['caption'],\n",
        "              padding='max_length',\n",
        "              truncation=True,\n",
        "              max_length=self.max_length,\n",
        "              return_tensors='pt'\n",
        "          )\n",
        "\n",
        "          # Tokenize target (next) tokens\n",
        "          target_tokens = self.tokenizer(\n",
        "              ann['next_tokens'],\n",
        "              padding='max_length',\n",
        "              truncation=True,\n",
        "              max_length=self.max_length,\n",
        "              return_tensors='pt'\n",
        "          )\n",
        "\n",
        "          return {\n",
        "              'image': image,\n",
        "              'input_ids': input_tokens['input_ids'].squeeze(0),\n",
        "              'attention_mask': input_tokens['attention_mask'].squeeze(0),\n",
        "              'labels': target_tokens['input_ids'].squeeze(0)\n",
        "          }\n",
        "      except IndexError as e:\n",
        "          print(f\"IndexError: {e}. Index {idx} is out of bounds.\")\n",
        "          raise\n",
        "      except KeyError as e:\n",
        "          print(f\"KeyError: {e}. Annotation entry is missing the expected key.\")\n",
        "          raise\n",
        "\n",
        "\n",
        "def train_one_epoch(model: torch.nn.Module,\n",
        "                    dataloader: DataLoader,\n",
        "                    optimizer: torch.optim.Optimizer,\n",
        "                    criterion: torch.nn.Module,\n",
        "                    device: str) -> float:\n",
        "    \"\"\"\n",
        "    Train the model for one epoch\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        # Move data to device\n",
        "        images = batch['image'].to(device)\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, input_ids)\n",
        "\n",
        "        # Calculate loss\n",
        "        # Reshape outputs and labels for cross entropy\n",
        "        print(outputs.shape, labels.shape)\n",
        "        # Use only the text portion of outputs for loss calculation\n",
        "        outputs_text_only = outputs[:, -512:, :]\n",
        "\n",
        "        B, S, V = outputs.shape  # Batch, Sequence, Vocab\n",
        "        loss = criterion(outputs_text_only.view(-1, V), labels.view(-1))\n",
        "\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def main():\n",
        "    # Example dataset structure\n",
        "    example_annotation = {\n",
        "        \"image_file\": \"dog_running.jpg\",\n",
        "        \"caption\": \"a photo of a dog running\",\n",
        "        \"next_tokens\": \"in the park\"\n",
        "    }\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", token=\"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\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = VisionLanguageDataset(\n",
        "        data_dir=\"images\",\n",
        "        annotations_file=\"annotations.json\",\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "    # Initialize model (assuming VisionLanguageModel from previous code)\n",
        "    model = VisionLanguageModel(\n",
        "        vocab_size=tokenizer.vocab_size,\n",
        "        image_embed_dim=768,\n",
        "        text_embed_dim=512,\n",
        "        nheads=8,\n",
        "        num_layers=6\n",
        "    )\n",
        "\n",
        "    # Training setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 10\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_loss = train_one_epoch(\n",
        "            model=model,\n",
        "            dataloader=dataloader,\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion,\n",
        "            device=device\n",
        "        )\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drSja46M1ot7",
        "outputId": "284c07d7-9aa6-437d-c836-ca5d83a4f958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Features Shape: torch.Size([1, 196, 512])\n",
            "Text Features Shape: torch.Size([1, 512, 512])\n",
            "Image2 Features Shape: torch.Size([1, 196, 512])\n",
            "Text2 Features Shape: torch.Size([1, 512, 512])\n",
            "MultiModalTransformer Input Shape: torch.Size([1, 708, 512])\n",
            "Fusion Features Shape: torch.Size([1, 708, 512])\n",
            "torch.Size([1, 708, 32000]) torch.Size([1, 512])\n",
            "Epoch 1/10, Average Loss: 10.6859\n",
            "Image Features Shape: torch.Size([1, 196, 512])\n",
            "Text Features Shape: torch.Size([1, 512, 512])\n",
            "Image2 Features Shape: torch.Size([1, 196, 512])\n",
            "Text2 Features Shape: torch.Size([1, 512, 512])\n",
            "MultiModalTransformer Input Shape: torch.Size([1, 708, 512])\n",
            "Fusion Features Shape: torch.Size([1, 708, 512])\n",
            "torch.Size([1, 708, 32000]) torch.Size([1, 512])\n",
            "Epoch 2/10, Average Loss: 7.5742\n",
            "Image Features Shape: torch.Size([1, 196, 512])\n",
            "Text Features Shape: torch.Size([1, 512, 512])\n",
            "Image2 Features Shape: torch.Size([1, 196, 512])\n",
            "Text2 Features Shape: torch.Size([1, 512, 512])\n",
            "MultiModalTransformer Input Shape: torch.Size([1, 708, 512])\n",
            "Fusion Features Shape: torch.Size([1, 708, 512])\n",
            "torch.Size([1, 708, 32000]) torch.Size([1, 512])\n",
            "Epoch 3/10, Average Loss: 5.7917\n",
            "Image Features Shape: torch.Size([1, 196, 512])\n",
            "Text Features Shape: torch.Size([1, 512, 512])\n",
            "Image2 Features Shape: torch.Size([1, 196, 512])\n",
            "Text2 Features Shape: torch.Size([1, 512, 512])\n",
            "MultiModalTransformer Input Shape: torch.Size([1, 708, 512])\n",
            "Fusion Features Shape: torch.Size([1, 708, 512])\n",
            "torch.Size([1, 708, 32000]) torch.Size([1, 512])\n",
            "Epoch 4/10, Average Loss: 4.6331\n",
            "Image Features Shape: torch.Size([1, 196, 512])\n",
            "Text Features Shape: torch.Size([1, 512, 512])\n",
            "Image2 Features Shape: torch.Size([1, 196, 512])\n",
            "Text2 Features Shape: torch.Size([1, 512, 512])\n",
            "MultiModalTransformer Input Shape: torch.Size([1, 708, 512])\n",
            "Fusion Features Shape: torch.Size([1, 708, 512])\n",
            "torch.Size([1, 708, 32000]) torch.Size([1, 512])\n",
            "Epoch 5/10, Average Loss: 3.6096\n",
            "Image Features Shape: torch.Size([1, 196, 512])\n",
            "Text Features Shape: torch.Size([1, 512, 512])\n",
            "Image2 Features Shape: torch.Size([1, 196, 512])\n",
            "Text2 Features Shape: torch.Size([1, 512, 512])\n",
            "MultiModalTransformer Input Shape: torch.Size([1, 708, 512])\n",
            "Fusion Features Shape: torch.Size([1, 708, 512])\n",
            "torch.Size([1, 708, 32000]) torch.Size([1, 512])\n",
            "Epoch 6/10, Average Loss: 2.9714\n",
            "Image Features Shape: torch.Size([1, 196, 512])\n",
            "Text Features Shape: torch.Size([1, 512, 512])\n",
            "Image2 Features Shape: torch.Size([1, 196, 512])\n",
            "Text2 Features Shape: torch.Size([1, 512, 512])\n",
            "MultiModalTransformer Input Shape: torch.Size([1, 708, 512])\n",
            "Fusion Features Shape: torch.Size([1, 708, 512])\n",
            "torch.Size([1, 708, 32000]) torch.Size([1, 512])\n",
            "Epoch 7/10, Average Loss: 2.3042\n",
            "Image Features Shape: torch.Size([1, 196, 512])\n",
            "Text Features Shape: torch.Size([1, 512, 512])\n",
            "Image2 Features Shape: torch.Size([1, 196, 512])\n",
            "Text2 Features Shape: torch.Size([1, 512, 512])\n",
            "MultiModalTransformer Input Shape: torch.Size([1, 708, 512])\n",
            "Fusion Features Shape: torch.Size([1, 708, 512])\n",
            "torch.Size([1, 708, 32000]) torch.Size([1, 512])\n",
            "Epoch 8/10, Average Loss: 1.6885\n",
            "Image Features Shape: torch.Size([1, 196, 512])\n",
            "Text Features Shape: torch.Size([1, 512, 512])\n",
            "Image2 Features Shape: torch.Size([1, 196, 512])\n",
            "Text2 Features Shape: torch.Size([1, 512, 512])\n",
            "MultiModalTransformer Input Shape: torch.Size([1, 708, 512])\n",
            "Fusion Features Shape: torch.Size([1, 708, 512])\n",
            "torch.Size([1, 708, 32000]) torch.Size([1, 512])\n",
            "Epoch 9/10, Average Loss: 1.3683\n",
            "Image Features Shape: torch.Size([1, 196, 512])\n",
            "Text Features Shape: torch.Size([1, 512, 512])\n",
            "Image2 Features Shape: torch.Size([1, 196, 512])\n",
            "Text2 Features Shape: torch.Size([1, 512, 512])\n",
            "MultiModalTransformer Input Shape: torch.Size([1, 708, 512])\n",
            "Fusion Features Shape: torch.Size([1, 708, 512])\n",
            "torch.Size([1, 708, 32000]) torch.Size([1, 512])\n",
            "Epoch 10/10, Average Loss: 0.9659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms  # Make sure this is torchvision.transforms\n",
        "from transformers import AutoTokenizer\n",
        "import json\n",
        "import os\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "\n",
        "def inference(model, tokenizer, image_path, text, transforms, max_length=512):\n",
        "    tokenized_text = tokenizer(\n",
        "        text,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transforms(image)\n",
        "    image = image.unsqueeze(0)\n",
        "\n",
        "    # Ensure input tokens are within vocab range before feeding to model\n",
        "    tokenized_text['input_ids'] = torch.clamp(tokenized_text['input_ids'],\n",
        "                                            min=0,\n",
        "                                            max=model.text_encoder.token_embedding.num_embeddings - 1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image, tokenized_text['input_ids'])\n",
        "\n",
        "    # Get predicted token IDs\n",
        "    # 1. Apply softmax\n",
        "    probabilities = torch.softmax(output[0, -1, :], dim=0)  # Get probabilities for the last token\n",
        "    # 2. Find the token with the highest probability\n",
        "    predicted_token_id = torch.argmax(probabilities).item()\n",
        "    # 3. Decode the token\n",
        "    predicted_token = tokenizer.decode(predicted_token_id)\n",
        "\n",
        "    return predicted_token\n",
        "\n",
        "# Define the transform outside the inference function\n",
        "transform = transforms.Compose([  # Use transforms.Compose directly\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "result = inference(model, tokenizer, \"images/dog.jpg\", 'dog', transform)  # Pass the transform object\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uN44cYjOCOE",
        "outputId": "556542a0-a080-4ab1-b194-e5e526d5195e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-man\n"
          ]
        }
      ]
    }
  ]
}